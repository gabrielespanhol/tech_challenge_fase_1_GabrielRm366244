
# ğŸ“š Tech Challenge API - Consulta de Livros

API pÃºblica desenvolvida como parte do Tech Challenge 2025 (RM366244) para ingestÃ£o, consulta e anÃ¡lise de informaÃ§Ãµes de livros, alÃ©m de integraÃ§Ã£o com modelos de Machine Learning.

---

## ğŸŒ URL DocumentaÃ§Ã£o

```
https://tech-challenge-fase-1-gabrielrm366244.onrender.com/docs
```

## ğŸŒ URL Dashboard

```
https://tech-challenge-fase-1-gabrielrm366244-wihb.onrender.com/
```
---

## Arquitetura do projeto

<div align="center">
    <img src="arquitetura.drawio.png" alt="Logo do Projeto" width="470" />
</div>

## 1. Fonte de Dados
Books to Scrape Ã© o site de onde os dados sÃ£o extraÃ­dos.

## 2. IngestÃ£o
A coleta de dados pode ser feita de duas maneiras:

Via API usando a rota /api/v1/scraping/trigger para iniciar o scraping remotamente.

Via script local, executado diretamente o arquivo web_scraping.py na pasta de scripts.

ApÃ³s a coleta, os dados sÃ£o enviados para a etapa de armazenamento.

## 3. Armazenamento
Os dados coletados podem ser salvos de duas formas:

PostgreSQL (Armazenado depois que a rota /api/v1/scraping/trigger Ã© disparada)

Arquivo CSV (Armazenado depois que o script web_scraping.py Ã© executado)

## 4. API ğŸ“– Endpoints da API

### ğŸ“¦ Books

#### ğŸ”„ Carregar Base de Dados
**POST** `/api/v1/scraping/trigger`  
> Dispara o scraping e popula a base de dados.

#### ğŸ—‘ï¸ Truncar Base de Dados
**DELETE** `/api/v1/scraping/trigger/delete`  
> Remove todos os registros da base.

#### ğŸ“š Listar Todos os Livros
**GET** `/api/v1/books`  
> Lista os livros com opÃ§Ã£o de `limit`.

#### ğŸ” Buscar Livros
**GET** `/api/v1/books/search`  
> Busca por `title` ou `category`.

#### â­ Listar Top Livros
**GET** `/api/v1/books/top-rated`  
> Lista livros com maior avaliaÃ§Ã£o.

#### ğŸ’° Filtrar Livros Por PreÃ§o
**GET** `/api/v1/books/price-range`  
> Filtros por `min` e `max` de preÃ§o.

#### ğŸ” Obter Livro por ID
**GET** `/api/v1/books/{id}`  
> Detalhes de um livro especÃ­fico.

#### ğŸ·ï¸ Listar Categorias
**GET** `/api/v1/categories`  
> Lista todas as categorias disponÃ­veis.

#### âœ… Verificar Status
**GET** `/api/v1/health`  
> Verifica se a API estÃ¡ funcionando.

#### ğŸ“ˆ Overview Geral
**GET** `/api/v1/stats/overview`  
> EstatÃ­sticas gerais sobre os livros.

#### ğŸ—‚ï¸ Stats por Categoria
**GET** `/api/v1/stats/categories`  
> EstatÃ­sticas agrupadas por categoria.

---

### ğŸ¤– Machine Learning

#### ğŸ”¢ Features para Modelos
**GET** `/api/v1/ml/features`  
> Dados preparados para uso como features.

#### ğŸ“‚ Dados para Treinamento
**GET** `/api/v1/ml/training-data`  
> Dataset formatado para treino de modelo.

#### ğŸ“¤ Enviar PrediÃ§Ãµes
**POST** `/api/v1/ml/predictions`  
> Recebe dados das prediÃ§Ãµes feitas pelo modelo.

#### ğŸ—ƒï¸ Listar PrediÃ§Ãµes
**GET** `/api/v1/ml/predictions-list`  
> Lista todas as prediÃ§Ãµes feitas.

---

### ğŸ“ˆ MÃ©tricas de Uso

#### â• Criar MÃ©trica
**POST** `/metrics/`  
> Armazena mÃ©trica de uso da API.

#### ğŸ“‹ Listar MÃ©tricas
**GET** `/metrics-list`  
> Retorna a lista de mÃ©tricas registradas.

---

## Como rodar o projeto: passo a passo

## 1. Clonar o repositÃ³rio

> git clone https://github.com/gabrielespanhol/tech_challenge_fase_1_GabrielRm366244.git<br>
> cd tech_challenge_fase_1_GabrielRm366244

## 2. Criar e ativar um ambiente virtual (recomendado)

> python -m venv venv<br>
> source venv/bin/activate   # no macOS/Linux<br>
> venv\Scripts\activate      # no Windows

## 3. Instalar dependÃªncias

> pip install -r requirements.txt<br>
> No arquivo requirements.txt, vocÃª encontrarÃ¡ as dependÃªncias necessÃ¡rias para rodar a aplicaÃ§Ã£o â€” por exemplo, FastAPI, BeautifulSoup, entre outras.


## 4. Preparar o banco de dados (opcional, conforme sua escolha)

> Os dados podem ser armazenados localmente via CSV ou em banco se vocÃª fizer web scraping pela rota da API<br> 
> Caso opte por fazer o web scraping pela rota da API Ã© necessario configurar o banco de dados de sua escolha no arquivo "db/session.py"<br> 
> O codigo para criar um banco SQLite local esta pronto, apenas precisa ser "descomentado" e a parte que aponta para uma base de produÃ§Ã£o precisa ser comentada

## 5. Rodar o scraping local (opcional)

> Para extrair os dados e salvÃ¡-los em CSV:<br>
> python scripts/web_scraping.py<br>
> Isso utiliza o BeautifulSoup para coletar dados de Books to Scrape e salvar localmente 

## 6. Iniciar a API

> Execute:<br>
> uvicorn main:app --reload<br>
> Isso iniciarÃ¡ a API com hot-reload localmente.


## ğŸ§¾ EspecificaÃ§Ã£o TÃ©cnica

- **VersÃ£o da API:** 1.0.0
- **DocumentaÃ§Ã£o OpenAPI:** (https://tech-challenge-fase-1-gabrielrm366244.onrender.com/docs)
- **Formato de resposta:** `application/json`

---

## ğŸ› ï¸ Tecnologias Utilizadas

- **Backend:** Python Â· FastAPI
- **Scraping:** BeautifulSoup
- **Banco de Dados:** PostgreSQL
- **Hospedagem:** Render
- **DocumentaÃ§Ã£o:** OpenAPI 3.1

---

## ğŸ‘¨â€ğŸ’» Desenvolvedor

**Gabriel Espanhol**  
RM: RM366244  

---

## ğŸ“Œ ObservaÃ§Ãµes

- Dados extraÃ­dos de [Books to Scrape](http://books.toscrape.com)
- API pÃºblica e educacional.
- Suporta anÃ¡lise estatÃ­stica, integraÃ§Ã£o com ML e visualizaÃ§Ã£o de mÃ©tricas.

---